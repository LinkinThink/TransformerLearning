{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#å®‰è£…æ‰€éœ€ä¾èµ–\n",
    "#!pip install -r requirements.txt\n",
    "#åµŒå…¥æ¨¡å‹ä¸‹è½½é“¾æ¥ï¼šhttps://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\n",
    "#ä¸‹è½½åè§£å‹"
   ],
   "id": "f05d3e9b9a0a32eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Transformerç»“æ„å›¾\n",
    "\n",
    "<img src=\"transformerç»“æ„.png\" alt=\"ç»“æ„å›¾\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "æ ¹æ®ç»“æ„ï¼Œå¯ä»¥å°†æ•´ä¸ªtransformeråˆ†æˆä»¥ä¸‹éƒ¨åˆ†ï¼š\n",
    "- è¾“å…¥\n",
    "    - è¯åµŒå…¥\n",
    "    - ä½ç½®ç¼–ç \n",
    "- Encoder\n",
    "    - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "    - æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–\n",
    "    - FFN\n",
    "- Decoder\n",
    "    - æ©ç è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "    - äº¤å‰æ³¨æ„åŠ›æœºåˆ¶\n",
    "    - å…¶ä½™ä¸Encoderç›¸åŒ"
   ],
   "id": "5b1539e448467276"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.è¯åµŒå…¥ä¸ä½ç½®ç¼–ç \n",
    "- è°ƒç”¨FastTextåµŒå…¥æ¨¡å‹\n",
    "- å°†å¥å­è½¬ä¸ºtokens,è½¬æˆå‘é‡\n",
    "- ç¼–å†™ä½ç½®ç¼–ç å‡½æ•°"
   ],
   "id": "8971172876caa66f"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# åŠ è½½ .vec æ–‡ä»¶ï¼ˆæ³¨æ„ binary=False ï¼‰\n",
    "#.vecæ–‡ä»¶æ˜¯æ–‡æœ¬æ ¼å¼ï¼Œå¯ç›´æ¥æŸ¥çœ‹ã€‚ ä½†ç”±äºæ•°æ®é‡å¤§ï¼Œè§£æé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œéœ€ç­‰å¾…æ¯”è¾ƒé•¿æ—¶é—´ï¼ˆåœ¨æˆ‘çš„ç”µè„‘ä¸Šæ˜¯3åˆ†åŠå·¦å³ï¼‰\n",
    "embedded_model = KeyedVectors.load_word2vec_format(\"cc.zh.300.vec\", binary=False)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49d5d624b7671886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:25:25.331579Z",
     "start_time": "2025-05-13T09:25:25.308829Z"
    }
   },
   "source": [
    "#.vecæ–‡ä»¶ç¬¬ä¸€è¡Œæ˜¯è¯æ±‡è¡¨å¤§å°å’Œæ¨¡å‹ç»´åº¦\n",
    "def get_vocab_size(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        print(f\"ç¬¬ä¸€è¡Œå†…å®¹ï¼š{first_line}\")\n",
    "        parts = first_line.split()\n",
    "        if len(parts) == 2:\n",
    "            vocab_size, vector_dim = map(int, parts)\n",
    "            print(f\"è¯æ±‡è¡¨å¤§å°: {vocab_size}\")\n",
    "            print(f\"è¯å‘é‡ç»´åº¦: {vector_dim}\")\n",
    "        else:\n",
    "            print(\"è¯¥æ–‡ä»¶å¯èƒ½æ²¡æœ‰å¤´éƒ¨ä¿¡æ¯ï¼Œéœ€è¦æ‰‹åŠ¨ç»Ÿè®¡ã€‚\")\n",
    "        return vocab_size, vector_dim\n",
    "\n",
    "\n",
    "vocab_size, d_model = get_vocab_size('cc.zh.300.vec')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€è¡Œå†…å®¹ï¼š2000000 300\n",
      "è¯æ±‡è¡¨å¤§å°: 2000000\n",
      "è¯å‘é‡ç»´åº¦: 300\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d290ff0a3a67d2c6",
   "metadata": {},
   "source": [
    "#1.1 æ¨¡å‹æµ‹è¯•ä¸åŸºæœ¬æ“ä½œ\n",
    "\n",
    "#æŸ¥è¯¢è¯å‘é‡\n",
    "word_vector = embedded_model[\"æ˜¥é£\"]\n",
    "print(f\"word vector shape: {word_vector.shape}\")\n",
    "\n",
    "#æ‰¾å‡ºäºæŒ‡å®šè¯æœ€ç›¸ä¼¼çš„5ä¸ªè¯\n",
    "print(f\"æœ€æ¥è¿‘çš„è¯:{embedded_model.most_similar('å›½ç‹', topn=5)}\")\n",
    "\n",
    "#è®¡ç®—ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "similarity = embedded_model.similarity('ç”·äºº', 'å¥³äºº')\n",
    "print(f\"ç›¸ä¼¼åº¦:{similarity}\")\n",
    "\n",
    "#ç±»æ¯”æ¨ç†  å›½ç‹ + å¥³äºº - ç”·äºº\n",
    "result = embedded_model.most_similar(positive=['å›½ç‹', 'å¥³äºº'], negative=['ç”·äºº'],topn=5)\n",
    "result\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f4700a22e6d3c2d",
   "metadata": {},
   "source": [
    "#å¯è§†åŒ–è¯å‘é‡  å°†è¯å‘é‡é™ç»´åˆ°2ç»´\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = ['king', 'queen', 'man', 'woman','dog','cat','apple','banana']\n",
    "vectors = [embedded_model[word] for word in words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(vectors)\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff952807edf4fb89",
   "metadata": {},
   "source": [
    "#1.2 åˆ†è¯ï¼Œè½¬æˆè¯å‘é‡\n",
    "import jieba\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def sentence_to_vectors_verbose(sentence, model):\n",
    "    #jiebaåˆ†è¯\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    vectors = []\n",
    "    tokens_info = []\n",
    "\n",
    "    #å°†åˆ†è¯ç»“æœåµŒå…¥æˆå‘é‡\n",
    "    for token in tokens:\n",
    "        token_vectors, token_labels = get_token_vector_recursive_verbose(token, model)\n",
    "        vectors.extend(token_vectors)\n",
    "        tokens_info.extend(token_labels)\n",
    "\n",
    "    return vectors, tokens_info\n",
    "\n",
    "def get_token_vector_recursive_verbose(token, model):\n",
    "    \"\"\"\n",
    "    é€’å½’è·å–tokenå‘é‡ï¼ŒåŒæ—¶è®°å½•æ˜¯ç›´æ¥å‘½ä¸­è¿˜æ˜¯ç»†åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    if token in model:\n",
    "        return [model[token]], [{'token': token, 'source': 'direct'}]\n",
    "    elif len(token) == 1:\n",
    "        return [], [{'token': token, 'source': 'OOV'}]  # å¯ä»¥é€‰æ‹©ç”¨UNK\n",
    "    else:\n",
    "        sub_vectors = []\n",
    "        sub_info = []\n",
    "        for char in token:\n",
    "            if char in model:\n",
    "                sub_vectors.append(model[char])\n",
    "                sub_info.append({'token': char, 'source': 'split'})\n",
    "            else:\n",
    "                sub_info.append({'token': char, 'source': 'OOV'})\n",
    "        return sub_vectors, sub_info\n",
    "\n",
    "\n",
    "# æµ‹è¯•å¥å­\n",
    "sentence = \"ä»Šå¤©å¤©æ°”ä¸é”™\"\n",
    "sentence_vectors, tokens_info = sentence_to_vectors_verbose(sentence, embedded_model)\n",
    "\n",
    "# å±•ç¤ºåˆ†è¯ä¸æ¥æº\n",
    "print(\"åˆ†è¯ä¸å¤„ç†ä¿¡æ¯ï¼š\")\n",
    "for info in tokens_info:\n",
    "    print(f\"{info['token']} - {info['source']}\")\n",
    "\n",
    "print(f\"\\næ€»å‘é‡æ•°: {len(sentence_vectors)}\")\n",
    "print(f\"æ¯ä¸ªå‘é‡ç»´åº¦: {sentence_vectors[0].size if sentence_vectors else 'æ— å‘é‡'}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebe4ac19fb78f4f",
   "metadata": {},
   "source": [
    "#1.3 ä½ç½®ç¼–ç \n",
    "import numpy as np\n",
    "\n",
    "#è¾“å…¥ä¸ºå¥å­é•¿åº¦ä¸å‘é‡ç»´åº¦\n",
    "def positional_encoding(seq_len, dim):\n",
    "    pos_enc = np.zeros((seq_len, dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, dim, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i) / dim)))\n",
    "            if i + 1 < dim:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / dim)))\n",
    "    return pos_enc  # shape: (seq_len, dim)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2eac7eb45e1e71bd",
   "metadata": {},
   "source": [
    "#å¯è§†åŒ–ä½ç½®ç¼–ç ç»“æœâ€”â€”ä½¿ç”¨çƒ­åŠ›å›¾ï¼Œä½ç»´æ•æ‰é«˜é¢‘ä¿¡æ¯é¢œè‰²å˜åŒ–å‰§çƒˆï¼Œåä¹‹é«˜ç»´é¢œè‰²å˜åŒ–ç¼“æ…¢\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_vectors(vectors):\n",
    "    # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(vectors, cmap='YlGnBu', cbar=True, xticklabels=10, yticklabels=10)\n",
    "    plt.title(\"Position Encoding Heatmap\")\n",
    "    plt.xlabel(\"Dimensions\")\n",
    "    plt.ylabel(\"Sequence Position\")\n",
    "    plt.show()\n",
    "\n",
    "# æµ‹è¯•,ç”Ÿæˆ50ä¸ªå¥å­ï¼Œç»´åº¦300\n",
    "plot_vectors(positional_encoding(50,300))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ebd13f49788a3d",
   "metadata": {},
   "source": [
    "#åŸå¥å­å‘é‡åŠ ä¸Šä½ç½®ç¼–ç \n",
    "embedded_vectors = sentence_vectors + positional_encoding(len(sentence_vectors), sentence_vectors[0].size)\n",
    "print(embedded_vectors.shape)\n",
    "\n",
    "#å°†è¾“å…¥æ ¼å¼è½¬æˆ(batch_size,sentence_len,dim),ä¸”è½¬æˆtorchä¸‹çš„tensor\n",
    "import torch\n",
    "embed_tensor = torch.tensor(embedded_vectors)\n",
    "embed_tensor = embed_tensor.unsqueeze(0).float()  #è¿™é‡Œè¾“å…¥åªæœ‰ä¸€å¥ï¼Œç›´æ¥å°†batch_sizeè®¾ä¸º1\n",
    "print(embed_tensor.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bebaa33aad0ff74d",
   "metadata": {},
   "source": [
    "## 2.å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\n",
    "- ç”ŸæˆQ,K,VçŸ©é˜µï¼Œæ‹†åˆ†å¤šå¤´\n",
    "- è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1caec7ce8be3ffd",
   "metadata": {},
   "source": [
    "æ³¨æ„åŠ›è®¡ç®—æ–¹å¼ï¼š $$ Attention = Softmax(\\frac{Q*K^T}{\\sqrt{dK}}) *V$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "47ad73ab584a9b86",
   "metadata": {},
   "source": [
    "#2.1 å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head,dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0,\"d_model å¿…é¡»èƒ½è¢«n_headæ•´é™¤\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head       #æ³¨æ„åŠ›å¤´æ•°\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #qkvï¼šè¾“å…¥çš„çº¿æ€§æ˜ å°„ oï¼šè¾“å‡º\n",
    "        self.q = nn.Linear(d_model,d_model)\n",
    "        self.k = nn.Linear(d_model,d_model)\n",
    "        self.v = nn.Linear(d_model,d_model)\n",
    "        self.o = nn.Linear(d_model,d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size ,seq_len ,d_model = x.size()\n",
    "        # print(f\"batch_size:{batch_size},seq_len:{seq_len},d_model:{d_model}\")\n",
    "\n",
    "        #æ‹†åˆ†ä¸ºå¤šå¤´ x.shape = (batch_size,seq_len,n_head,d_model),è¿™é‡Œå…ˆå°†d_modelæ‹†åˆ†æˆn_head*head_dim,ä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œå†å°†seq_lenå’Œn_headçš„ä½ç½®äº¤æ¢\n",
    "        #æœ€ç»ˆæ ¼å¼æ˜¯(batch_size,n_head,seq_len,head_dim)\n",
    "        Q = self.q(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        K = self.k(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        V = self.v(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "\n",
    "        #è®¡ç®—æ³¨æ„åŠ›\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = F.dropout(attention_scores)  #dropout,å‚è€ƒã€ŠAttention is all you needã€‹åŸæ–‡ï¼Œé˜²æ­¢æ³¨æ„åŠ›è¿‡æ‹Ÿåˆ\n",
    "        attention_output = torch.matmul(attention_scores, V)\n",
    "\n",
    "        #åˆå¹¶ ä¸æ‹†åˆ†åŒç†ï¼Œè¦æ³¨æ„åœ¨.viewå’Œ.transposeåå¼ é‡å˜ä¸ºå†…å­˜ä¸è¿ç»­ï¼ˆ.viewä¼šæ”¹å˜ç´¢å¼•æ–¹å¼ï¼‰ï¼Œæ‰€ä»¥éœ€è¦.contiguousè¿›è¡Œè¿ç»­åŒ–\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size,seq_len, self.d_model)\n",
    "\n",
    "        output = self.o(attention_output)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5a188dcea38280f",
   "metadata": {},
   "source": [
    "multi_attention = MultiHeadSelfAttention(d_model=embed_tensor.shape[2],n_head=5)\n",
    "atten_output = multi_attention(embed_tensor)\n",
    "print(atten_output.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30a1dd491c465bee",
   "metadata": {},
   "source": [
    "## 3.æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–\n",
    "- å±‚å½’ä¸€åŒ–åŸç†\n",
    "- Pytorchæ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc299164b9c29569",
   "metadata": {},
   "source": [
    "å±‚å½’ä¸€åŒ–åŸç†:\n",
    "- å‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡ï¼š$$ x = [x_1,x_2,...,x_H]$$ $$ Î¼ = \\frac{1}{H} \\sum_{i=1}^{H} x_i $$ $$Ïƒ = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(x_i - Î¼)^2 + Ïµ} $$\n",
    "- å½’ä¸€åŒ–åï¼š $$\\hat x_i = \\frac{x_i - Î¼}{Ïƒ}$$\n",
    "- å†è¿›è¡Œç¼©æ”¾å¹³ç§»ï¼š$$ y_i =  Î³ \\hat x_i + Î²$$\n",
    "- å…¶ä¸­ï¼šğ›¾,ğ›½ æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚Ïµ æ˜¯ä¸€ä¸ªé˜²æ­¢é™¤ 0 çš„å°å€¼"
   ]
  },
  {
   "cell_type": "code",
   "id": "73bfb1fd67febcd3",
   "metadata": {},
   "source": [
    "def layer_norm_manual(x, eps=1e-5):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    std = x.std(dim=-1, keepdim=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "x = torch.randn(32, 10, 512)\n",
    "output = layer_norm_manual(x)\n",
    "print(output.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "311ed7350a4df9ea",
   "metadata": {},
   "source": [
    "#å®è·µä¸­ä¸»è¦è°ƒç”¨pytorchå°è£…çš„å‡½æ•°\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å‡è®¾è¾“å…¥ (batch_size, seq_len, hidden_size)\n",
    "x = torch.randn(32, 10, 512)\n",
    "\n",
    "# åˆ›å»ºLayerNormï¼ˆå¯¹æœ€åä¸€ç»´åšå½’ä¸€åŒ–ï¼‰\n",
    "layer_norm = nn.LayerNorm(normalized_shape=512)\n",
    "\n",
    "# ä½¿ç”¨\n",
    "output = layer_norm(x)\n",
    "print(output.shape)  # torch.Size([32, 10, 512])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2d9a58bf1fa4474",
   "metadata": {},
   "source": [
    "#æ®‹å·®è¿æ¥åˆ™æ˜¯å°†ä¸Šä¸€å±‚è¾“å‡ºè¿æ¥åˆ°å½“å‰å±‚è¾“å‡ºï¼ˆç›´æ¥ç›¸åŠ ï¼‰\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self,size,dropout=0.1):\n",
    "        super(ResidualLayerNorm,self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x,sublayer_output):\n",
    "        x = self.norm(x + self.dropout(sublayer_output))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed3dcee382a1cc8e",
   "metadata": {},
   "source": [
    "## 4.FFNå‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "- ç»“æ„ï¼š$$FFN(x) = ReLu(xW_1 + b1)W_2 + b2 $$\n",
    "- å…¶ä¸­ï¼š$W_1$é€šå¸¸æ˜¯4å€äºè¾“å…¥ç»´åº¦ï¼Œç”¨æ¥å°†è¾“å…¥éçº¿æ€§æ˜ å°„åˆ°æ›´é«˜ç»´çš„ä¿¡æ¯ä»¥è·å¾—æ›´ä¸°å¯Œçš„è¯­ä¹‰è¡¨è¾¾ï¼Œ$W_2$é™ç»´å›åŸæ¥ç»´åº¦ï¼ŒReLuæ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0dac009ff87efff",
   "metadata": {},
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model,d_ff)  #d_ffï¼šéšè—å±‚ç»´åº¦ï¼Œé€šå¸¸å–4å€d_model\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w2 = nn.Linear(d_ff,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7af665af2ea5ef",
   "metadata": {},
   "source": [
    "## 5.Encoder,å„æ¨¡å—å°è£…æ•´åˆ"
   ]
  },
  {
   "cell_type": "code",
   "id": "47b1ec5aa899e030",
   "metadata": {},
   "source": [
    "class Transformer_encoder(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,n_head,dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(d_model,n_head,dropout=dropout)\n",
    "        self.ffn = FFN(d_model,d_ff,dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten_output = self.attention(x)\n",
    "        x = self.layer_norm_1(x + self.dropout(atten_output))\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm_2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f03765c505de0c3",
   "metadata": {},
   "source": [
    "#ç»“æ„å›¾ä¸­ï¼ŒEncoderéœ€è¦å°†ä¸Šè¿°æ¨¡å—é‡å¤Næ¬¡\n",
    "#è¿™é‡Œä½¿ç”¨ModuleListå®ç°\n",
    "class TransformerEncoderStack(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, d_ff, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Transformer_encoder(d_model, d_ff, n_head, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bfe9c28943987f6",
   "metadata": {},
   "source": [
    "#å¯è§†åŒ–å¼ é‡ï¼Œçƒ­åŠ›å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def visualize_tensor_stages(*tensors, labels=None, title='Tensor Evolution Heatmap'):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å¤šä¸ªé˜¶æ®µçš„ [batch, seq_len, d_model] å¼ é‡ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - *tensors: ä»»æ„æ•°é‡çš„å¼ é‡ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º [batch, seq_len, d_model]\n",
    "    - labels: æ¯ä¸ªå¼ é‡çš„æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºæ ‡é¢˜æ˜¾ç¤º\n",
    "    \"\"\"\n",
    "    num_tensors = len(tensors)\n",
    "    if labels is None:\n",
    "        labels = [f'Stage {i}' for i in range(num_tensors)]\n",
    "\n",
    "    batch_idx = 0  # é»˜è®¤åªå–ç¬¬ä¸€ä¸ª batch å¯è§†åŒ–\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_tensors, figsize=(6 * num_tensors, 5))\n",
    "    if num_tensors == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        assert tensor.dim() == 3, f\"Tensor {i} must have shape [batch, seq_len, d_model]\"\n",
    "        data = tensor[batch_idx].detach().cpu().numpy()  # shape: [seq_len, d_model]\n",
    "\n",
    "        sns.heatmap(data, ax=axes[i], cmap='viridis')\n",
    "        axes[i].set_xlabel('Dimension (d_model)')\n",
    "        axes[i].set_ylabel('Token Position (seq_len)')\n",
    "        axes[i].set_title(labels[i])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#è°ƒç”¨æ¡ˆä¾‹\n",
    "# visualize_tensor_stages(embedding_out, attn_out, resnorm_out,\n",
    "#                         labels=[\"Embedding\", \"Attention Output\", \"Post-ResNorm\"])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e173af42c5293bf0",
   "metadata": {},
   "source": [
    "encoder = TransformerEncoderStack(n_layers=6, d_model=d_model, d_ff=4*d_model, n_head=5, dropout=0.1)\n",
    "encoder_output = encoder(embed_tensor)\n",
    "visualize_tensor_stages(embed_tensor,atten_output,encoder_output\n",
    "                        ,labels=[\"Embedding\",\"Attention\",\"Encoder_output\"])\n",
    "print(encoder_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1cb156c5e03e149",
   "metadata": {},
   "source": [
    "## 6.æ©ç è‡ªæ³¨æ„åŠ›\n",
    "- ç”Ÿæˆæ©ç çŸ©é˜µï¼Œæ©ç›–å¯¹æœªæ¥å€¼çš„è¯¢é—®"
   ]
  },
  {
   "cell_type": "code",
   "id": "a61d15c0919989e8",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head,dropout=0.1,visual=False):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0,\"d_model å¿…é¡»èƒ½è¢«n_headæ•´é™¤\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #å¯è§†åŒ–å¼€å…³\n",
    "        self.visual = visual\n",
    "\n",
    "        #çº¿æ€§æ˜ å°„\n",
    "        self.q = nn.Linear(d_model,d_model)\n",
    "        self.k = nn.Linear(d_model,d_model)\n",
    "        self.v = nn.Linear(d_model,d_model)\n",
    "        self.o = nn.Linear(d_model,d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size ,seq_len ,d_model = x.size()\n",
    "\n",
    "        Q = self.q(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        K = self.k(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        V = self.v(x).view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1, 2)\n",
    "\n",
    "        #è®¡ç®—æ³¨æ„åŠ›\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
    "        # ç”Ÿæˆæ©ç çŸ©é˜µ (ä¸‹ä¸‰è§’çŸ©é˜µï¼šmaskæœªæ¥ä¿¡æ¯)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).repeat(batch_size, 1, 1)  # (batch_size, seq_len, seq_len)\n",
    "        mask = (mask == 0)  # 0çš„éƒ¨åˆ†ä¸ºmaskçš„åŒºåŸŸ\n",
    "        # åº”ç”¨æ©ç \n",
    "        attention_scores = attention_scores.masked_fill(mask == True, -1e9)  # æ©ç›–ä¸Šä¸‰è§’\n",
    "        #å¯è§†åŒ–æ©ç›–ç»“æœ\n",
    "        if self.visual:\n",
    "            visualize_tensor_stages(attention_scores[-1],title=\"masking\")\n",
    "            print(attention_scores)\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attention_scores = F.dropout(attention_scores)  #dropout\n",
    "        attention_output = torch.matmul(attention_scores, V)\n",
    "\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size,seq_len, self.d_model)\n",
    "\n",
    "        output = self.o(attention_output)\n",
    "        return output\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ccaf7515604aa700",
   "metadata": {},
   "source": [
    "mask_test = MaskedSelfAttention(d_model=d_model,n_head=5,dropout=0.1,visual=True)\n",
    "mask_test_output = mask_test(embed_tensor)\n",
    "# print(mask_test_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2f271c92d4b1b7f",
   "metadata": {},
   "source": [
    "## 7.äº¤å‰æ³¨æ„åŠ›æœºåˆ¶\n",
    "- ç”Ÿæˆå½“å‰çš„æŸ¥è¯¢Qä¸Encoderè¾“å‡ºçš„K,V"
   ]
  },
  {
   "cell_type": "code",
   "id": "f86936193b1468c0",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"d_model å¿…é¡»èƒ½è¢«n_headæ•´é™¤\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q = nn.Linear(d_model, d_model)  # Decoder Query\n",
    "        self.k = nn.Linear(d_model, d_model)  # Encoder Key\n",
    "        self.v = nn.Linear(d_model, d_model)  # Encoder Value\n",
    "        self.o = nn.Linear(d_model, d_model)  # Output\n",
    "\n",
    "    #è¾“å…¥æ˜¯å‰é¢encoderçš„è¾“å‡ºå’Œdecoderå‰ä¸€å±‚çš„è¾“å‡º\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        batch_size, seq_len_dec, d_model = x_dec.size()\n",
    "        batch_size, seq_len_enc, d_model  = x_enc.size()\n",
    "        # ç”Ÿæˆ decoderçš„æŸ¥è¯¢çŸ©é˜µQ\n",
    "        Q = self.q(x_dec).view(batch_size, seq_len_dec, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        #ç”Ÿæˆ encoderçš„é”®ä¸å€¼K,V\n",
    "        K = self.k(x_enc).view(batch_size, seq_len_enc, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        V = self.v(x_enc).view(batch_size, seq_len_enc, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # è®¡ç®—äº¤å‰æ³¨æ„åŠ›å¾—åˆ†\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, n_head, seq_len, seq_len)\n",
    "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # è·å–æ³¨æ„åŠ›è¾“å‡º\n",
    "        attention_output = torch.matmul(attention_probs, V)  # (batch_size, n_head, seq_len, head_dim)\n",
    "\n",
    "        # åˆå¹¶å¤šå¤´è¾“å‡º\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len_dec, self.d_model)\n",
    "\n",
    "        # çº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡º\n",
    "        output = self.o(attention_output)\n",
    "        return output\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "debb7fedca7de7d9",
   "metadata": {},
   "source": [
    "#æ–°åµŒå…¥\n",
    "newsentence = \"æ˜å¤©å¤©æ°”å¦‚ä½•\"\n",
    "\n",
    "new_sen_vector , _ = sentence_to_vectors_verbose(newsentence,embedded_model)\n",
    "\n",
    "#ä½ç½®ç¼–ç \n",
    "new_embedding = new_sen_vector + positional_encoding(len(new_sen_vector),d_model)\n",
    "# #è½¬tensor\n",
    "new_embedding_tensor = torch.tensor(new_embedding)\n",
    "new_embedding_tensor = new_embedding_tensor.unsqueeze(0).float()      # shape: (1, seq_len, 300)\n",
    "\n",
    "new_embedding_tensor.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfdaa5f083e9e3b0",
   "metadata": {},
   "source": [
    "mask_atten = MaskedSelfAttention(new_embedding_tensor.shape[-1],n_head=5,dropout=0.1,visual=True)\n",
    "cross_atten = CrossAttention(new_embedding_tensor.shape[-1],n_head=5,dropout=0.1)\n",
    "#\n",
    "mask_atten_output = mask_atten(new_embedding_tensor)\n",
    "cross_atten_output = cross_atten(encoder_output, mask_atten_output)\n",
    "print(cross_atten_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0964e19c0956551",
   "metadata": {},
   "source": [
    "## 8.æ•´åˆDecoderéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "id": "382781875b4f7a9d",
   "metadata": {},
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,n_head,dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_selfAttention = MaskedSelfAttention(d_model=d_model,n_head=n_head,dropout=dropout)\n",
    "        self.cross_attention = CrossAttention(d_model=d_model,n_head=n_head,dropout=dropout)\n",
    "        self.ffn = FFN(d_model,d_ff,dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_enc,x):\n",
    "        mask_attention = self.mask_selfAttention(x)\n",
    "        x = self.layer_norm_1(x + self.dropout(mask_attention))\n",
    "        cross_attention = self.cross_attention(x_enc,x)\n",
    "        x = self.layer_norm_2(x + self.dropout(cross_attention))\n",
    "        ffn = self.ffn(x)\n",
    "        x = self.layer_norm_3(x + self.dropout(ffn))\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9135b2e773a66f6",
   "metadata": {},
   "source": [
    "class TransformerDecoderStack(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,n_head,n_layer,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoder(d_model, d_ff, n_head, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x_enc,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x_enc,x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e8a0583675ba875",
   "metadata": {},
   "source": [
    "decoder = TransformerDecoderStack(n_layer=6,d_model=d_model,d_ff=4*d_model,n_head=5,dropout=0.1)\n",
    "decoder_output = decoder(encoder_output,new_embedding_tensor)\n",
    "visualize_tensor_stages(encoder_output,new_embedding_tensor)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "873a4dc71d805d6d",
   "metadata": {},
   "source": [
    "## 9.æœ€ç»ˆè¾“å‡º\n",
    "- è¾“å‡ºå±‚ï¼ˆçº¿æ€§æ˜ å°„ï¼‰ï¼Œå¤§å°ï¼ˆè¯å‘é‡ç»´åº¦d_model,è¯æ±‡è¡¨å¤§å°vocab_sizeï¼‰\n",
    "- æ¨ç†ä¸è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "id": "5843ba0a6788c673",
   "metadata": {},
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self,n_layers,d_model,d_ff,n_head,vocab_size,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoderStack(n_layers=n_layers,d_model=d_model,d_ff=d_ff,n_head=n_head,dropout=dropout)\n",
    "        self.decoder = TransformerDecoderStack(n_layer=n_layers,d_model=d_model,d_ff=d_ff,n_head=n_head,dropout=dropout)\n",
    "        self.fc_out = nn.Linear(d_model,vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, src,trg):\n",
    "        enc_output = self.encoder(src)\n",
    "        dec_output = self.decoder(enc_output,trg)\n",
    "\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13d6b538e6d76acb",
   "metadata": {},
   "source": [
    "t = MyTransformer(n_layers=6,d_model=d_model,d_ff=4*d_model,n_head=5,dropout=0.1,vocab_size=vocab_size)\n",
    "prob_out = t(embed_tensor,new_embedding_tensor)\n",
    "print(prob_out.shape)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
